""" Receiver Operating Characteristics module containing ROC data computations."""

from __future__ import annotations

import os

import pandas as pd
import numpy as np
from scipy import interpolate, optimize
from collections import deque

from sklearn import metrics
from tools.plotter_helper import Plotter
import random


class RocUtil:
    """
    Computation utilities class for ROC curve
    """

    @staticmethod
    def confusion_matrix(ground_truth: list, prediction: list) -> (int, int, int, int):
        """
        Method computes confusion (CF) matrix based on sklearn method
        and returns in all cases all four elements of the CF matrix.
        It provides robust wrapper for scikit-learn method that is not
        always returning correct matrix dataset.
        Args:
            ground_truth: list containing ground truth values
            prediction:  list containing prediction values.
        Returns:
            (tn, - TN - True negative count,
             fp, - FP - False positive count,
             fn, - FN - False negative count,
             tp) - TP - True positive count
        """
        # Confusion matrix components TP, FP, TN, FN initialization with zeros
        tn, fp, fn, tp = 0, 0, 0, 0

        # Compute confusion matrix
        cf_matrix = metrics.confusion_matrix(ground_truth, prediction)

        '''
        # If confusion matrix only contains one value, reshape it
        if cf_matrix.size == 1:
            cf_matrix = cf_matrix.reshape(1, 1)

        # Compute TP, FP, TN, FN based on the shape of confusion_matrix
        if cf_matrix.shape == (1, 1):
            # Only positive class present
            if np.unique(ground_truth)[0] == 1:
                tp = cf_matrix[0, 0]
            # Only negative class present
            elif np.unique(ground_truth)[0] == 0:
                tn = cf_matrix[0, 0]
        else:
            # All classes present
            tn, fp, fn, tp = cf_matrix.ravel()
        '''

        # All classes present
        tn, fp, fn, tp = cf_matrix.ravel()

        return tn, fp, fn, tp

    @staticmethod
    def normalize(
        data: pd.DataFrame,
        enable_line: bool,
        cfg: dict,
        logger: object = None,
        fpr_label: str = "fpr",
        tpr_label: str = "tpr",
        recall_label: str = "recall",
        precision_label: str = "prec",
    ) -> (pd.DataFrame, bool):
        """
        Method normalizes TPR/FPR and Precision/Recall parameters by
        adding start and stop points if they don't exist in the data set due
        to narrow or AD model infeasibility search.
        Args:
            data: original TPR/FPR dataset
            enable_line: flag to determine on how to process ROC curve
            cfg: main experiment configuration
            logger: logger object
            fpr_label: False Positive Rate column label name in the data set
            tpr_label: True Positive Rate column label name in the data set
            recall_label: Recall column label name in the data set
            precision_label: Precision column label name in the data set
        Returns:
            <pd.DataFrame> - normalized dataframe
            <bool> - is_monotonic check flag
        """
        # Check if there are missing points
        if data.empty or fpr_label not in data.columns or tpr_label not in data.columns:
            if not data.empty:
                roc_error_file = (f"{cfg['global']['path']}experiments/{cfg['experiment']['path']}"
                                  f"error/roc_m_failed_{random.randint(1, 90000):05d}.png")

                data.to_csv(f"{roc_error_file[:-4]}roc.csv")
                logger.error("Data is missing FPR/TPR columns")
            else:
                logger.error("Input data empty")

            return pd.DataFrame(), False

        # Cleanup process with simple operations to replace N/A drop duplicates
        # and sorting ROC curve generated by the parallel process.
        cp_data = data.copy(deep=True)

        cp_data.sort_values(by=[fpr_label, tpr_label], inplace=True)

        # Remove fpr/tpr pair duplicates
        cp_data.drop_duplicates(subset=[fpr_label, tpr_label], keep="first", inplace=True)

        # Round up results
        cp_data[fpr_label] = cp_data[fpr_label].round(decimals=4)
        cp_data[tpr_label] = cp_data[tpr_label].round(decimals=4)

        # Find min/max tau values so in case of CD ROC values will be correctly sorted
        min_tau = 0
        max_tau = 0
        if "tau" in cp_data.columns:
            if cp_data.shape[0] > 0:
                min_tau = min(cp_data.loc[:, "tau"]) - 1
                min_tau = 0 if min_tau < 0 else min_tau
                max_tau = max(cp_data.loc[:, "tau"]) + 1
            else:
                logger.warn("Potential problem with tau column")

        # Preform checks whether ROC has ending points (due to parameters search limit
        # this might not be possible to achieve). If it doesn't exist create it.

        # Start of the ROC (0, 0)
        if (not cp_data.loc[:, fpr_label].tolist()[0]
            == cp_data.loc[:, tpr_label].tolist()[0]
            == 0):
            row = {
                "tau": max_tau,
                "tp": 0,
                "tn": 0,
                "fp": 0,
                "fn": 0,
                fpr_label: 0,
                tpr_label: 0,
                recall_label: 1,
                precision_label: 0,
                "auc": 0.0,
            }
            cp_data = pd.concat([pd.DataFrame(row, index=[0]), cp_data]).reset_index(drop=True)
            cp_data.sort_values(by=[fpr_label, tpr_label], inplace=True)

        # End of the ROC (1, 1)
        if (not cp_data.loc[:, fpr_label].tolist()[-1]
            == cp_data.loc[:, tpr_label].tolist()[-1]
            == 1):
            row = {
                "tau": min_tau,
                "tp": 0,
                "tn": 0,
                "fp": 0,
                "fn": 0,
                fpr_label: 1,
                tpr_label: 1,
                recall_label: 0,
                precision_label: 1,
                "auc": 0.0,
            }

            cp_data = (pd.concat([cp_data,
                                 pd.DataFrame(row, index=[cp_data.index[-1] + 1])])
                       .reset_index(drop=True))
            cp_data.sort_values(by=[fpr_label, tpr_label], inplace=True)

        cp_data.fillna(0.0, inplace=True)

        monotonic_check = RocUtil.is_monotonic(cp_data.loc[:, tpr_label].tolist())

        # Check monotonicity of the graph
        if not monotonic_check and cfg["model"]["debug"]:
            roc_error_file = (f"{cfg['global']['path']}experiments/{cfg['experiment']['path']}"
                              f"error/roc_m_failed_{random.randint(1, 90000):05d}.png")
            logger.warn(f"Monotonic check FAILED >> {roc_error_file}")
            cp_data.to_csv(f"{roc_error_file[:-3]}.csv", index=False)
            Plotter.roc_2d(cp_data.loc[:, ["fpr", "tpr"]],
                           {"auc": 0.0},
                           "ROC FAILED",
                           roc_error_file,
                           0.03,
                           enable_line)

        return cp_data, monotonic_check

    @staticmethod
    def is_monotonic(data: list) -> bool:
        """
        Method validates whether data is monotonic
        Args:
            data:   list containing input elements for monotonic check
        Returns:
            <bool> - True if data is monotonic, otherwise False
        """
        result = True
        # Check if list needs to be reversed
        n_data = data
        if n_data[0] < n_data[-1]:
            n_data = n_data[::-1]

        for i, d_point in enumerate(n_data):
            if i == len(n_data) - 1:
                break
            m_diff = d_point - n_data[i + 1]
            if m_diff < 0:
                result = False
                break
        return result

    @staticmethod
    def extract_value(x: object) -> float:
        """
        Method extracts a value from either pd.DataFrame or pd.Series
        Args:
            x: input object
        Returns:
            <float>
        """
        if isinstance(x, pd.DataFrame):
            x = x[0]
        if isinstance(x, pd.Series):
            x = x.tolist()[0]
        return x

    @staticmethod
    def opt_detection_box(data: pd.DataFrame, n_percent: float) -> list:
        """
        Method computes optimal ROC detection box
        using linear approximation and ROC data and given
        optimal percentage limit.
        Args:
            data: input ROC data
            n_percent: N percent value expressed between 0 and 1
        Returns:
            <list> - [[x1, y1], [x2, y2]] optimal box marking optimal detection
                     area on ROC curve.
        """

        # Sort data to ensure correct area selection
        data_srt = data.sort_values(by=["fpr", "tpr"])

        # ROC(fpr=0)
        z_set = data_srt[data_srt["fpr"] == 0]
        z_set = z_set[z_set["tpr"] == z_set["tpr"].max()]

        # ROC (fpr=N+1)
        n_set = data_srt[data_srt["fpr"] > n_percent]

        # Compute linear approximation
        # polyfit would work
        x1 = z_set["fpr"][z_set.index[0]]
        y1 = z_set["tpr"][z_set.index[0]]

        s_set = data_srt[data_srt["fpr"] < n_percent]
        s_set = s_set[s_set["tpr"] == s_set["tpr"].max()]

        x11 = s_set["fpr"][s_set.index[-1]]
        y11 = s_set["tpr"][s_set.index[-1]]

        x2 = n_set["fpr"][n_set.index[0]]
        y2 = n_set["tpr"][n_set.index[0]]

        x1 = RocUtil.extract_value(x1)
        y1 = RocUtil.extract_value(y1)

        x11 = RocUtil.extract_value(x11)
        y11 = RocUtil.extract_value(y11)

        x2 = RocUtil.extract_value(x2)
        y2 = RocUtil.extract_value(y2)

        x = np.array([float(x11), float(x2)])
        y = np.array([float(y11), float(y2)])

        return [[x1, y1], [n_percent, np.interp(n_percent, x, y)]]
